prefix:

user:
  create: true
  email: admin@sentry.local
  password: aaaa

images:
  sentry:
    repository: getsentry/sentry
    tag: 20.7.1
    pullPolicy: IfNotPresent
    # imagePullSecrets: []
  snuba:
    repository: getsentry/snuba
    tag: e944bc414bfc97c9836febbfb2f310a3237f91cf
    pullPolicy: IfNotPresent
    # imagePullSecrets: []

sentry:
  web:
    replicas: 1
    env: {}
    probeInitialDelaySeconds: 10
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

    autoscaling:
      enabled: false
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 50

  worker:
    replicas: 3
    # concurrency: 4
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

    # it's better to use prometheus adapter and scale based on
    # the size of the rabbitmq queue
    autoscaling:
      enabled: false
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 50

  cron:
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []
  postProcessForward:
    replicas: 1
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

snuba:
  api:
    replicas: 1
    env: {}
    probeInitialDelaySeconds: 10
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

    autoscaling:
      enabled: false
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 50

  consumer:
    replicas: 1
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

  outcomesConsumer:
    replicas: 1
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

  sessionsConsumer:
    replicas: 1
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

  replacer:
    env: {}
    resources: {}
    affinity: {}
    nodeSelector: {}
    # tolerations: []
    # podLabels: []

  dbInitJob:
    env: {}

  migrateJob:
    env: {}


hooks:
  enabled: true
  dbInit:
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 300m
        memory: 2048Mi
  snubaInit:
    resources:
      limits:
        cpu: 2000m
        memory: 1Gi
      requests:
        cpu: 700m
        memory: 1Gi
system:
  url: ""
  adminEmail: ""
  secretKey: 'icLq77rCyY_qrMMpXa6TQNjkDV6mU!c'
  public: false #  This should only be used if you’re installing Sentry behind your company’s firewall.

mail:
  backend: dummy # smtp
  useTls: false
  username: ""
  password: ""
  port: 25
  host: ""
  from: ""

symbolicator:
  enabled: false

auth:
  register: true

service:
  name: sentry
  type: ClusterIP
  externalPort: 9000
  annotations: {}
  # externalIPs:
  # - 192.168.0.1
  # loadBalancerSourceRanges: []

github: {} # https://github.com/settings/apps (Create a Github App)
# github:
#   appId: "xxxx"
#   appName: MyAppName
#   clientId: "xxxxx"
#   clientSecret: "xxxxx"
#   privateKey: "-----BEGIN RSA PRIVATE KEY-----\nMIIEpA" !!!! Don't forget a trailing \n
#   webhookSecret:  "xxxxx`"

githubSso: {} # https://github.com/settings/developers (Create a OAuth App)
  # clientId: "xx"
  # clientSecret: "xx"

slack: {}
# slack:
#   clientId:
#   clientSecret:
#   verificationToken:

ingress:
  enabled: false
  # annotations:
  #   kubernetes.io/tls-acme:
  #   certmanager.k8s.io/issuer:
  #   nginx.ingress.kubernetes.io/proxy-body-size:
  #
  # hostname:
  #
  # tls:
  # - secretName:
  #   hosts:

filestore:
  # Set to one of filesystem, gcs or s3 as supported by Sentry.
  backend: filesystem

  filesystem:
    path: /var/lib/sentry/files

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: true
      ## database data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      accessMode: ReadWriteOnce
      size: 10Gi

      ## Whether to mount the persistent volume to the Sentry worker and
      ## cron deployments. This setting needs to be enabled for some advanced
      ## Sentry features, such as private source maps. If you disable this
      ## setting, the Sentry workers will not have access to artifacts you upload
      ## through the web deployment.
      ## Please note that you may need to change your accessMode to ReadWriteMany
      ## if you plan on having the web, worker and cron deployments run on
      ## different nodes.
      persistentWorkers: false

  ## Point this at a pre-configured secret containing a service account. The resulting
  ## secret will be mounted at /var/run/secrets/google
  gcs:
    # credentialsFile: credentials.json
    #  secretName:
    #  bucketName:

  ## Currently unconfigured and changing this has no impact on the template configuration.
  s3: {}
  #  accessKey:
  #  secretKey:
  #  bucketName:
  #  endpointUrl:
  #  signature_version:
  #  region_name:
  #  default_acl:

config:
  configYml: |
    # No YAML Extension Config Given
  sentryConfPy: |
    # No Python Extension Config Given
  snubaSettingsPy: |
    # No Python Extension Config Given


clickhouse:
  enabled: true
  clickhouse:
    imageVersion: "19.16"
    configmap:
      remote_servers:
        internal_replication: true
    persistentVolumeClaim:
      enabled: true
      dataPersistentVolume:
        enabled: true
        accessModes:
        - "ReadWriteOnce"
        storage: "30Gi"

## This value is only used when clickhouse.enabled is set to false
##
externalClickhouse:
  ## Hostname or ip address of external clickhouse
  ##
  host: "clickhouse"
  tcpPort: 9000
  ## Cluster name, can be found in config
  ## (https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-remote-servers)
  ## or by executing `select * from system.clusters`
  ##
  # clusterName: test_shard_localhost

kafka:
  enabled: true
  replicaCount: 3
  allowPlaintextListener: true
  defaultReplicationFactor: 3
  offsetsTopicReplicationFactor: 3
  transactionStateLogReplicationFactor: 3
  transactionStateLogMinIsr: 3

  service:
    port: 9092

## This value is only used when kafka.enabled is set to false
##
externalKafka:
  ## Hostname or ip address of external kafka
  ##
  # host: "kafka-confluent"
  port: 9092

redis:
  enabled: true
  nameOverride: sentry-redis
  usePassword: false
  ## Just omit the password field if your redis cluster doesn't use password
  # password: redis
  master:
    persistence:
      enabled: true

## This value is only used when redis.enabled is set to false
##
externalRedis:
  ## Hostname or ip address of external redis cluster
  ##
  # host: "redis"
  port: 6379
  ## Just omit the password field if your redis cluster doesn't use password
  # password: redis

postgresql:
  enabled: true
  nameOverride: sentry-postgresql
  postgresqlUsername: postgres
  postgresqlDatabase: sentry
  replication:
    enabled: false
    slaveReplicas: 2
    synchronousCommit: "on"
    numSynchronousReplicas: 1

## This value is only used when postgresql.enabled is set to false
##
externalPostgresql:
  # host: postgres
  port: 5432
  username: postgres
  # password: postgres
  database: sentry
  # sslMode: require

rabbitmq:
  ## If disabled, Redis will be used instead as the broker.
  enabled: true
  forceBoot: true
  replicaCount: 3
  rabbitmqErlangCookie: pHgpy3Q6adTskzAT6bLHCFqFTF7lMxhA
  rabbitmqUsername: guest
  rabbitmqPassword: guest
  nameOverride: ""

  podDisruptionBudget:
    minAvailable: 1

  persistentVolume:
    enabled: true
  resources: {}
  # rabbitmqMemoryHighWatermark: 600MB
  # rabbitmqMemoryHighWatermarkType: absolute

  definitions:
    policies: |-
     {
       "name": "ha-all",
       "pattern": "^((?!celeryev.*).)*$",
       "vhost": "/",
       "definition": {
         "ha-mode": "all",
         "ha-sync-mode": "automatic",
         "ha-sync-batch-size": 1
       }
     }

## Prometheus Exporter / Metrics
##
metrics:
  enabled: false

  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  ## Metrics exporter resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 100m
  #     memory: 100Mi

  nodeSelector: {}
  tolerations: []
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  service:
    type: ClusterIP
    labels: {}

  image:
    repository: prom/statsd-exporter
    tag: v0.17.0
    pullPolicy: IfNotPresent

  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    enabled: false
    additionalLabels: {}
    namespace: ""
    namespaceSelector: {}
    # Default: scrape .Release.Namespace only
    # To scrape all, use the following:
    # namespaceSelector:
    #   any: true
    scrapeInterval: 30s
    # honorLabels: true